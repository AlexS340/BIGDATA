# Проект для ETL процессов датасета [фильмов](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset)
## Гипотезы, которые мы хотим проверить:
- Бюджет фильма влияет на рейтинг
- Жанр фильма влияет на рейтинг
- Продолжительность фильма влияет на рейтинг


## Guide по запуску
### Подготовка окружения
Первым делом необходимо скачать датасет с kaggle ([ссылка](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset)) и распаковать архив в любое место.
Затем скачать и установить [docker desktop](https://www.docker.com/products/docker-desktop/)
Клонируем этот репозиторий к себе в удобное место `git clone https://github.com/AlexS340/BIGDATA.git` и ввести логопасс.
__Важно!__ В папке со склонированным проектом нужно создать папки `dags/` `logs/` `plugins/` `config/` (пустыми) и `data/` с файлами датасета
### Сборка образа для airflow
Поскольку "чистый" образ airflow не несет в себе некоторых библиотек и инструментов - лучше собрать кастомный образ самостоятельно. Для этого в терминале необходимо перейти в папку с проектом (в терминале `cd /путь/до/проекта/`) и запустить 
```bash
docker build -t extending_airflow:latest .
```

Название уже вшито в `docker-compose.yaml`, поэтому после билда кастомного образа airflow можно сразу поднимать все необходимые для работы сервисы 
```bash
docker-compose up
```
В docker desktop можно проверять статус поднятых контейнеров во вкладке containers

### Настройка окружения
Теперь переходим в веб-сервисы http://localhost:8080/ (airflow-web)
и http://localhost:5050/ (pgAdmin4, для коннекта в бд)

Далее:
**В pgAdmin:**
- При первом входе создаем пароль администратора (любой, главное не забыть)
- Настраиваем новое подключение:
-- Servers -> ПКМ -> register -> server
-- Указываем в появившемся окошке имя сервера (любое), переходим во вкладку _connection_
-- Указываем:
```yaml
hostname = postgres
port = 5432
maintenance database = postgres
user = postgres
password = airflow
```
- Создаем в подключенном сервере новую БД:
-- databases -> ПКМ -> create -> Database -> называем `movies`

### Запуск пайплайна
Чтобы запустить обработку, откроем airflow, в поиске найдем даг `ETL_process`, провалимся в него и нажмем `trigger dag`. В открывшемся окошке вводим путь `data/`(до датасета на компе) и запускаем даг.

После того, как все выполнится, в папке `data/` появятся все изображения, а в _postgres_ все сырые и процесснутые таблицы
